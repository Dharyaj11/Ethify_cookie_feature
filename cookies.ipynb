{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pattern string', 'label', 'Pattern type', 'url'], dtype='object')\n",
      "Accuracy: 0.67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dark_pattern_vectorizer.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from joblib import dump\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "selected_classification = \"label\"  # Assuming \"label\" is your column with 0 or 1\n",
    "\n",
    "# Load your CSV file with the correct encoding\n",
    "df = pd.read_csv('cookies.csv', encoding='Windows-1252')\n",
    "\n",
    "# Print the column names to identify the correct one\n",
    "print(df.columns)\n",
    "\n",
    "# Assuming \"Pattern String\" is your column with the text from the cookies\n",
    "df = df[pd.notnull(df[\"pattern string\"])]\n",
    "col = [\"pattern string\", selected_classification]\n",
    "df = df[col]\n",
    "\n",
    "# Data preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['pattern string'] = df['pattern string'].apply(preprocess_text)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['pattern string'], df[selected_classification], train_size=.8, random_state=42\n",
    ")\n",
    "\n",
    "# Vectorize and transform the text data\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Build and train the model\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(count_vect.transform(X_test))\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = metrics.accuracy_score(y_pred, y_test)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Save the model and vectorizer\n",
    "dump(clf, 'dark_pattern_classifier.joblib')\n",
    "dump(count_vect, 'dark_pattern_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 0.0\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "\n",
    "# Load the trained model and vectorizer\n",
    "loaded_clf = load('dark_pattern_classifier.joblib')\n",
    "loaded_vectorizer = load('dark_pattern_vectorizer.joblib')\n",
    "\n",
    "pattern_string =\"This Site uses cookies and similar technologies, including third-party cookies, to function properly, perform statistical analysis, offer you a better experience and send our online advertising messages in line with your preferences. Consult the Cookie Policy  to find out more, to know which cookies are used and how to disable them and/or to withhold your consent.\"\n",
    "\n",
    "# Preprocess the test pattern string\n",
    "test_pattern = preprocess_text(pattern_string)\n",
    "\n",
    "# Vectorize and transform the test data\n",
    "test_pattern_counts = loaded_vectorizer.transform([test_pattern])\n",
    "test_pattern_tfidf = tfidf_transformer.transform(test_pattern_counts)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = loaded_clf.predict(test_pattern_tfidf)\n",
    "\n",
    "# Print the result\n",
    "print(f'Predicted Label: {prediction[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from joblib import load\n",
    "\n",
    "# Load the trained model and vectorizer\n",
    "loaded_clf = load('dark_pattern_classifier.joblib')\n",
    "loaded_vectorizer = load('dark_pattern_vectorizer.joblib')\n",
    "\n",
    "# Preprocess function for cookie information\n",
    "def preprocess_cookies(cookies):\n",
    "    # Combine cookie names and values into a single string\n",
    "    cookie_text = ' '.join([f'{cookie.name}={cookie.value}' for cookie in cookies])\n",
    "    # Apply the same preprocessing function used for training\n",
    "    preprocessed_cookie_text = preprocess_text(cookie_text)\n",
    "    return preprocessed_cookie_text\n",
    "\n",
    "def extract_cookies_from_website(url):\n",
    "    # Make a GET request to the URL\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error: Unable to fetch website content. {e}')\n",
    "        return None\n",
    "\n",
    "    # Parse HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract and preprocess the cookies\n",
    "    cookies = response.cookies\n",
    "    preprocessed_cookies = preprocess_cookies(cookies)\n",
    "\n",
    "    # Vectorize and transform the cookie data\n",
    "    cookie_counts = loaded_vectorizer.transform([preprocessed_cookies])\n",
    "    cookie_tfidf = tfidf_transformer.transform(cookie_counts)\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = loaded_clf.predict(cookie_tfidf)\n",
    "\n",
    "    # Check if the predicted label is 1 and inject JavaScript into the HTML\n",
    "    if prediction[0] == 1:\n",
    "        script = \"\"\"\n",
    "        <script>\n",
    "            alert(\"Dark pattern detected on this website!\");\n",
    "        </script>\n",
    "        \"\"\"\n",
    "        soup.head.append(BeautifulSoup(script, 'html.parser'))\n",
    "\n",
    "    # Return the modified HTML content\n",
    "    return str(soup)\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://www.oneplus.in/\"\n",
    "modified_html = extract_cookies_from_website(url)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
